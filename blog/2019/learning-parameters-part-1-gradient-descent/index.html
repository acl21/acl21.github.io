<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h4><a href="https://towardsdatascience.com/tagged/learning-parameters" rel="external nofollow noopener" target="_blank">Learning Parameters</a></h4> <h4>Gradient Descent is an iterative optimization algorithm for finding the (local) minimum of a function.</h4> <p>Gradient Descent is one of the most popular techniques in optimization, very commonly used in training neural networks. It is intuitive and explainable, given the right background of essential Calculus. Take a look at this blog post of mine — <a href="https://towardsdatascience.com/learning-parameters-part-0-5cfffd647bdc" rel="external nofollow noopener" target="_blank">Part 0</a> of sorts, that covers some of the prerequisites needed to make better sense of this series. You can check out all the posts in the <strong><em>Learning Parameters</em></strong> series by clicking on the kicker tagged at the top of this post.</p> <p>In this blog post, we build up the motivation for Gradient Descent using a toy neural network. We also derive Gradient Descent update rule from scratch and interpret what happens geometrically using the same toy neural network.</p> <blockquote>Citation Note: Most of the content and figures in this blog are directly taken from Lecture 5 of <a href="https://www.cse.iitm.ac.in/~miteshk/CS7015.html" rel="external nofollow noopener" target="_blank">CS7015: Deep Learning</a> course offered by <a href="https://www.cse.iitm.ac.in/~miteshk/" rel="external nofollow noopener" target="_blank">Prof. Mitesh Khapra</a> at IIT-Madras.</blockquote> <h3>A Trite Metaphor</h3> <p>Imagine you’re standing on a mountain, there could be a lot of possible paths available to you to descend. Gradient Descent (GD) in a nutshell gives you a principled way of descending the mountain.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/465/1*i3Gsdnr4srsBoLhPVoEUWQ.png"></figure> <p>GD encourages you first to find a direction in which the mountain has the steepest ascent and asks you to go quite the opposite to that. You might argue that this seemingly simple idea, might not always work. You are right, GD could make you walk slow even when the surface is flat (when you can run), but we will address the limitations later at the end of the post.</p> <h3>Motivation</h3> <p>Taking the metaphor forward to a toy example, let’s say we want to train a toy neural network with just one neuron. And the premise is as follows:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/962/1*ZBDrFo0woUzqNPjP0TMs-Q.png"></figure> <p>The training objective is to find the best combination of <strong><em>w</em></strong><em> and </em><strong><em>b </em></strong>that makes function<em> </em><strong><em>L(w, b)</em></strong><em> </em>output its<em> </em>minimum value<em>.</em></p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/378/1*HaUe7o3HKmyS-JpdgS_iuQ.png"></figure> <p>Now, what does it mean to train a network? Suppose we train the toy network with (<em>x, y</em>)<em> = </em>(0.5, 0.2)<em> </em>and (2.5, 0.9), at the end of the training, we expect to find <strong><em>w*</em></strong> and <strong><em>b*</em></strong> such that<em> f</em>(0.5) outputs 0.2 and <em>f</em>(2.5) outputs 0.9. We hope to see a sigmoid function such that (0.5, 0.2) and (2.5, 0.9) lie on the sigmoid.</p> <h4>Brute Force Stuff You Will Never Do</h4> <p>Can we try to find such a <strong><em>w*</em></strong>, <strong><em>b*</em></strong> manually? <br>Let us try a random guess.. (say, w = 0.5, b = 0)</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*NkCdVlIPhoMnVhW-Ntu4Kw.png"></figure> <p>Let’s keep guessing. Shall we?</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/929/1*bj0KLAu8bluOe1AxIu2VNA.png"></figure> <h4>Geometric Interpretation Of Stuff You Will Never Do</h4> <p>Can we visualize the guesswork? We can! Since we have only 2 points and 2 parameters <strong><em>(w, b)</em></strong> we can easily plot <strong><em>L(w, b)</em></strong> for different values of <strong><em>(w, b)</em></strong> and pick the one where <strong><em>L(w, b)</em></strong> is minimum.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/449/1*rg7pGh5YSNlm2ghoum83wA.png"></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/984/1*LPxbwM51k2iXDsM5vrqcpw.gif"></figure> <p>But of course, this becomes intractable once you have many more data points and many more parameters! Further, here we have plotted the error surface only for a small range of <strong><em>(w, b), </em></strong>from (−6, 6) and not from (−inf, inf). Gradient Descent to the rescue!</p> <h3>Gradient Descent</h3> <p>If it’s not already clear, the task at hand is all about finding the best combination of parameters that minimize the loss function. Gradient Descent gives us a principled way of traversing the error surface so that we can reach the minimum value quickly without resorting to brute force search.</p> <h4>Deriving The Gradient Descent Rule</h4> <p>It is reasonable to randomly initialize <strong><em>w</em></strong> and <strong><em>b</em></strong> and then update them iteratively in the <strong>best way</strong> possible to reach our goal of minimizing the loss function. Let us mathematically define this.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/798/1*jKpqYCSvhEEv-jynCwNKdQ.png"></figure> <p>But how do we find the most ‘desirable’ change in <em>θ</em>? What is the right <strong>Δ</strong><em>θ</em> to use? The answer comes from the <a href="http://fourier.eng.hmc.edu/e176/lectures/NM/node45.html" rel="external nofollow noopener" target="_blank"><strong>Taylor Series</strong></a>.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/964/1*9B5TFde4a7Yh0Yt35nsBeA.png"></figure> <p>This means the direction <em>u </em>or<em> </em><strong>Δ</strong><em>θ </em>that we intend to move in should be at 180-degree angle w.r.t. the gradient. At a given point on the loss surface, we move in the direction opposite to the gradient of the loss function at that point. This is the golden Gradient Descent Rule!!</p> <h4>Weight/Parameter Update Rule</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*imRrzCELAOGiGHPg6YMlsg.png"></figure> <h4>Algorithm</h4> <p>Now that we have a more principled way of moving in the <strong><em>w-b</em></strong><em> </em>plane than our brute force algorithm. Let’s create an algorithm from this rule…</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/479/1*Gnc1n67aNhaqaByWe-y5HA.png"></figure> <h3>Gradient Descent In Action</h3> <p>To see GD in practice, we first have to derive ∇<strong><em>w</em></strong> and ∇<strong><em>b</em></strong> for our toy neural network. If you work it out, you’ll see that they are as follows:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/653/1*RjdP7GPIZjtsj3YQUn5oNA.png"></figure> <h4>Python Code</h4> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/b9be719720e007b31764234a93cda0b4/href" rel="external nofollow noopener" target="_blank">https://medium.com/media/b9be719720e007b31764234a93cda0b4/href</a></iframe> <h4>Geometric Interpretation</h4> <p>Let’s start from a random point on the error surface and look at the updates.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/492/1*ghMZkEOArRtRVGOFMu4aOQ.gif"></figure> <h3>Limitation</h3> <p>Gradient Descent in its raw form has an obvious drawback. Let look at an example curve <em>f(x) = x² + 1,</em> shown below:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/319/1*_V2aSf3uNzbWEy_JRYak-A.png"></figure> <ul> <li>When the curve is steep the gradient (∇y1/∇x1) is large.</li> <li>When the curve is gentle (∇y2/∇x2) is small.</li> </ul> <p>Recall that our weight updates are proportional to the gradient <strong><em>w = w — η∇w</em></strong><em>. </em>Hence, in the regions where the curve is gentle, the updates are small whereas in the regions where the curve is steep the updates are large. Let’s see what happens when we start from a different point on the surface.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/454/1*qJ4t5n_9-6AnpOufeYBmMA.gif"></figure> <p>Irrespective of where we start from, once we hit a surface which has a gentle slope, the progress slows down. Due to this, training methods could take forever to converge.</p> <h3>Conclusion</h3> <p>In this blog post, we made an argument to emphasize on the need of Gradient Descent using a toy neural network. We also derived Gradient Descent update rule from scratch and interpreted what goes on with each update geometrically using the same toy neural network. We also addressed a significant limitation of GD, issue of slowing down at gentle regions, in its raw form with a curve illustration. <em>Momentum-Based Gradient Descent</em> overcomes this drawback to an extent by letting the ‘momentum’ of recent gradient updates control the update magnitude in the current step.</p> <p>Read all about it in the next post of this series at:</p> <ul><li><a href="https://towardsdatascience.com/learning-parameters-part-2-a190bef2d12" rel="external nofollow noopener" target="_blank">Learning Parameters, Part 2: Momentum-Based And Nesterov Accelerated Gradient Descent</a></li></ul> <h3>Acknowledgment</h3> <p>A lot of credit goes to <a href="https://www.cse.iitm.ac.in/~miteshk/" rel="external nofollow noopener" target="_blank"><strong>Prof. Mitesh M Khapra</strong></a> and the TAs of <a href="https://www.cse.iitm.ac.in/~miteshk/CS7015.html" rel="external nofollow noopener" target="_blank"><strong>CS7015: Deep Learning</strong></a><strong> </strong>course by IIT Madras for such rich content and creative visualizations. I merely just compiled the provided lecture notes and lecture videos concisely.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=eb3e8bb9ffbb" width="1" height="1" alt="">&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/data-science/learning-parameters-part-1-eb3e8bb9ffbb" rel="external nofollow noopener" target="_blank">Learning Parameters, Part 1: Gradient Descent</a> was originally published in <a href="https://medium.com/data-science" rel="external nofollow noopener" target="_blank">TDS Archive</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p> </body></html>