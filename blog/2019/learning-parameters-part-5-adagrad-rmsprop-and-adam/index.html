<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h4><a href="https://medium.com/tag/learning-parameters/latest" rel="external nofollow noopener" target="_blank">Learning Parameters</a></h4> <h3>Learning Parameters, Part 5: AdaGrad, RMSProp, and Adam</h3> <h4>Let’s look at gradient descent with an adaptive learning rate.</h4> <p>In <a href="https://towardsdatascience.com/learning-parameters-part-4-6a18d1d3000b" rel="external nofollow noopener" target="_blank">part 4</a>, we looked at some heuristics that can help us tune the learning rate and momentum better. In this final article of the <a href="https://medium.com/tag/learning-parameters/latest" rel="external nofollow noopener" target="_blank">series</a>, let us look at a more <em>principled</em> way of adjusting the learning rate and give the learning rate a chance to adapt.</p> <blockquote>Citation Note: Most of the content and figures in this blog are directly taken from Lecture 5 of <a href="https://www.cse.iitm.ac.in/~miteshk/CS7015.html" rel="external nofollow noopener" target="_blank">CS7015: Deep Learning</a> course offered by <a href="https://www.cse.iitm.ac.in/~miteshk/" rel="external nofollow noopener" target="_blank">Prof. Mitesh Khapra</a> at IIT-Madras.</blockquote> <h4>Motivation for Adaptive Learning Rate</h4> <p>Consider the following simple perceptron network with sigmoid activation.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/607/1*CRj9U6_LBVMFEaceunB0CA.png"></figure> <p>It should be easy to see that given a single point (<strong>x</strong>, <em>y</em>), gradients of <strong>w </strong>would be the following:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/625/1*8t9xxheID3mR741SMGBxwA.png"><figcaption>See <a href="https://towardsdatascience.com/learning-parameters-part-1-eb3e8bb9ffbb" rel="external nofollow noopener" target="_blank">Part-1</a> if this is unclear.</figcaption></figure> <p>Gradient of <strong><em>f(x)</em></strong><em> </em>w.r.t to a particular weight is clearly dependent on its corresponding input. If there are <em>n</em> points, we can just sum the gradients over all the <em>n</em> points to get the total gradient. This news is neither new nor special. But what would happen if the feature <strong><em>x2 </em></strong>is very sparse (i.e., if its value is 0 for most inputs)? It is fair to assume that ∇<strong><em>w2</em></strong> will be 0 for most inputs (see formula) and hence <strong><em>w2</em></strong> will not get enough updates.</p> <p>Why should that bother us though? It is important to note that if at all <strong><em>x2</em></strong> is both sparse as well as important, we would want to take the updates to <strong><em>w2</em> </strong>seriously. To make sure updates happen even when a particular input is sparse, can we have a different learning rate for each parameter which takes care of the frequency of the features? We sure can. I mean that is the whole point of this article.</p> <h3><strong>AdaGrad — Adaptive Gradient Algorithm</strong></h3> <h4>Intuition</h4> <p>Decay the learning rate for parameters in proportion to their update history (more updates means more decay).</p> <h4>Update Rule for AdaGrad</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/327/1*KN7IkA5J5ZpQ6LjLrtmZwA.png"></figure> <p>It is clear from the update rule that history of the gradient is accumulated in <strong><em>v</em></strong>. The smaller the gradient accumulated, the smaller the <strong><em>v</em></strong> value will be, leading to a bigger learning rate (because <strong><em>v</em></strong> divides <em>η</em>).</p> <h4>Python Code for AdaGrad</h4> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/942b7c5162578113b707cb997c478c13/href" rel="external nofollow noopener" target="_blank">https://medium.com/media/942b7c5162578113b707cb997c478c13/href</a></iframe> <h4>AdaGrad in Action</h4> <p>To see AdaGrad in action, we need to first create some data where one of the features is sparse. How would we do this to the toy network we used across all parts of the Learning Parameters series? Well, our network has just two parameters (<strong><em>w</em></strong> and <strong><em>b</em></strong>, see <em>Motivation </em>in <a href="https://towardsdatascience.com/learning-parameters-part-1-eb3e8bb9ffbb" rel="external nofollow noopener" target="_blank">part-1</a>). Of these, the input/feature corresponding to b is always on, so we can’t really make it sparse. So the only option is to make x sparse. Which is why we created 100 random <em>(x,y)</em> pairs and then roughly 80% of these pairs we set <em>x</em> to 0, making the feature for <strong><em>w</em></strong> sparse.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/599/1*g8b6r_qAQC4NiJ2h_UFsuw.png"><figcaption>Vanilla GD(black), momentum (red), NAG (blue)</figcaption></figure> <p>Before we actually look at AdaGrad in action, please look at the other 3 optimizers above - vanilla GD(black), momentum (red), NAG (blue). There is something interesting that these 3 optimizers are doing for this dataset. Can you spot it? Feel free to pause and ponder. <strong>Answer:</strong> Initially, all three algorithms are moving mainly along the vertical (<strong><em>b</em></strong>) axis and there is very little movement along the horizontal (<strong><em>w</em></strong>) axis. Why? Because in our data, the feature corresponding to <strong><em>w</em></strong> is sparse and hence <strong><em>w</em></strong> undergoes very few updates. On the other hand, <strong><em>b</em></strong> is very dense and undergoes many updates. Such sparsity is very common in large neural networks containing 1000s of input features and hence we need to address it. Let us now look at AdaGrad in action.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/448/1*aflJY_KSjorG1YHNiz7Rnw.gif"></figure> <p>Voila! By using a parameter specific learning rate AdaGrad ensures that despite sparsity <strong><em>w</em></strong> gets a higher learning rate and hence larger updates. Furthermore, it also ensures that if <strong><em>b</em></strong> undergoes a lot of updates, its effective learning rate decreases because of the growing denominator. In practice, this does not work so well if we remove the square root from the denominator (something to ponder about). What’s the flipside? Over time the effective learning rate for <strong><em>b</em></strong> will decay to an extent that there will be no further updates to <strong><em>b</em></strong>. Can we avoid this? RMSProp can!</p> <h3>RMSProp — <strong>Root Mean Square Propagation</strong> </h3> <h4>Intuition</h4> <p>AdaGrad decays the learning rate very aggressively (as the denominator grows). As a result, after a while, the frequent parameters will start receiving very small updates because of the decayed learning rate. To avoid this why not decay the denominator and prevent its rapid growth.</p> <h4>Update Rule for RMSProp</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/371/1*oTk6bl5ccXp9540bFv8WXg.png"></figure> <p>Everything is very similar to AdaGrad, except now we decay the denominator as well.</p> <h4>Python Code for RMSProp</h4> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/c5df21bfe487f20a162b485a4dad3ed5/href" rel="external nofollow noopener" target="_blank">https://medium.com/media/c5df21bfe487f20a162b485a4dad3ed5/href</a></iframe> <h4>RMSProp in Action</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/488/1*rn3JEQkOB3Znob1Y_EHakg.gif"><figcaption>Vanilla GD(black), momentum (red), NAG (blue), AdaGrad (magenta)</figcaption></figure> <p>What do you see? How is RMSProp different from AdaGrad? Feel free to pause and ponder. <strong>Answer:</strong> AdaGrad got stuck when it was close to convergence, it was no longer able to move in the vertical (<strong><em>b</em></strong>) direction<br>because of the decayed learning rate. RMSProp overcomes this problem by being less aggressive on the decay.</p> <h3>Adam — <strong>Adaptive Moment Estimation</strong> </h3> <h4><strong>Intuition</strong></h4> <p>Do everything that RMSProp does to solve the denominator decay problem of AdaGrad. In addition to that, use a cumulative history of gradients.</p> <h4>Update Rule for Adam</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/538/1*Qzpf8aKwdBYTgMuL69C5qw.png"></figure> <p>and a similar set of equations for <strong><em>b</em></strong><em>_t</em>. Notice that the update rule for Adam is very similar to RMSProp, except we look at the cumulative history of gradients as well (<strong><em>m</em></strong><em>_t</em>). Note that the third step in the update rule above is bias correction. Explanation by Prof. Mitesh M Khapra on why bias correction is necessary can be found <a href="https://www.youtube.com/watch?v=-0ZMU-gnm2g" rel="external nofollow noopener" target="_blank">here</a>.</p> <h4>Python Code for Adam</h4> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/965d202d6fc0b9b58c4ad7514b870b90/href" rel="external nofollow noopener" target="_blank">https://medium.com/media/965d202d6fc0b9b58c4ad7514b870b90/href</a></iframe> <h4>Adam in Action</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/488/1*m9JMNb9z2bzFlmPfK0WGUQ.gif"></figure> <p>Quite clearly, taking a cumulative history of gradients speeds it up. For this toy dataset, it appears to be overshooting (a little) but even then it converges way faster than the other optimizers.</p> <h3>Million Dollar Question: Which algorithm to use?</h3> <ul> <li>Adam seems to be more or less the default choice now (<em>β1</em> = 0.9, <em>β2</em> = 0.999 and <em>ϵ</em> = 1e − 8 ).</li> <li>Although it is supposed to be robust to initial learning rates, we have observed that for sequence generation problems <em>η</em> = 0.001, 0.0001 works best.</li> <li>Having said that, many papers report that SGD with momentum (Nesterov or classical) with a simple annealing learning rate schedule also works well in practice (typically, starting with <em>η</em> = 0.001, 0.0001 for sequence generation problems).</li> <li>Adam might just be the best choice overall.</li> <li>Some recent work suggests that there is a problem with Adam and it will not converge in some cases.</li> </ul> <h3>Conclusion</h3> <p>In this final article of the series, we looked at how gradient descent with adaptive learning rate can help speed up convergence in neural networks. Intuition, python code and visual illustration of three widely used optimizers — AdaGrad, RMSProp, and Adam are covered in this article. Adam combines the best properties of RMSProp and AdaGrad to work well even with noisy or sparse datasets.</p> <h3>Acknowledgment</h3> <p>A lot of credit goes to <a href="https://www.cse.iitm.ac.in/~miteshk/" rel="external nofollow noopener" target="_blank"><strong>Prof. Mitesh M Khapra</strong></a> and the TAs of <a href="https://www.cse.iitm.ac.in/~miteshk/CS7015.html" rel="external nofollow noopener" target="_blank"><strong>CS7015: Deep Learning</strong></a><strong> </strong>course by IIT Madras for such rich content and creative visualizations. I merely just compiled the provided lecture notes and lecture videos concisely.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/816/1*x0_9l2EDmWXxkqCNxKoWfA.png"><figcaption><strong>Source:</strong> Paperspace article on RMSProp by <a href="https://blog.paperspace.com/intro-to-optimization-momentum-rmsprop-adam/" rel="external nofollow noopener" target="_blank">Ayoosh Kathuria</a>.</figcaption></figure> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=65a2f3583f7d" width="1" height="1" alt="">&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/data-science/learning-parameters-part-5-65a2f3583f7d" rel="external nofollow noopener" target="_blank">Learning Parameters Part 5: AdaGrad, RMSProp, and Adam</a> was originally published in <a href="https://medium.com/data-science" rel="external nofollow noopener" target="_blank">TDS Archive</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p> </body></html>