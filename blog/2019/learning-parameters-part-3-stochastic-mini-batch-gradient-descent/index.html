<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h4><a href="https://medium.com/tag/learning-parameters/latest" rel="external nofollow noopener" target="_blank">Learning Parameters</a></h4> <h4>Let’s digress a bit from optimizers and talk about the stochastic<br>versions of these algorithms.</h4> <p>In part 2, we looked at two useful variants of gradient descent — <a href="https://towardsdatascience.com/learning-parameters-part-2-a190bef2d12" rel="external nofollow noopener" target="_blank">Momentum-Based and Nesterov Accelerated Gradient Descent</a>. In this post, we are going to look at stochastic versions of gradient descent. You can check out all the posts in the <strong><em>Learning Parameters </em></strong>series by clicking on the kicker tag at the top of this post.</p> <blockquote>Citation Note: Most of the content and figures in this blog are directly taken from Lecture 5 of <a href="https://www.cse.iitm.ac.in/~miteshk/CS7015.html" rel="external nofollow noopener" target="_blank">CS7015: Deep Learning</a> course offered by <a href="https://www.cse.iitm.ac.in/~miteshk/" rel="external nofollow noopener" target="_blank">Prof. Mitesh Khapra</a> at IIT-Madras.</blockquote> <h3>Motivation</h3> <p>Let us look at vanilla gradient descent we talked about in part-1 of the series.</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/6f6b3d0514e51110ee07f8afc381d719/href" rel="external nofollow noopener" target="_blank">https://medium.com/media/6f6b3d0514e51110ee07f8afc381d719/href</a></iframe> <p>If you observe the block of code between the two weirdly commented lines, you notice that the gradient is calculated across the entire dataset. Meaning, the algorithm goes over the entire data once before updating the parameters. Why? Because this is the true gradient of the loss as derived earlier in part-1 (the sum of the gradients of the losses corresponding to each data point). This is a good thing as we are not approximating anything. Hence, all theoretical assumptions and guarantees hold (in other words each step guarantees that the loss will decrease). Is this desirable? Sure it is, but what is the flipside?</p> <p>Imagine we have a million points in the training data. To make 1 update to <strong><em>w</em></strong>, <strong><em>b</em></strong> the algorithm makes a million calculations. Obviously, this could be very slow!! Can we do something better? Yes, let’s look at stochastic gradient descent.</p> <h3>Stochastic Gradient Descent</h3> <p>Stochastic gradient descent<strong> </strong>(often shortened to SGD) is an iterative method for optimizing a differentiable objective function, a stochastic approximation of gradient descent optimization. Basically, you are going with an approximation of some sort instead of the noble ‘true gradient’. Stochastic gradient descent is the dominant method used to train deep learning models. Let’s straightaway look at the code for SGD.</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/3aca6b1e6362712f57c10ecb8e8f6049/href" rel="external nofollow noopener" target="_blank">https://medium.com/media/3aca6b1e6362712f57c10ecb8e8f6049/href</a></iframe> <p>Here’s what’s going on — instead of making an update by calculating gradients for all the data points, we make an update with gradients of just one data point at a time. Thus, the name stochastic, as we are estimating the total gradient based on a single data point. Almost like tossing a coin only once and estimating P(heads). Now if we have a million data points we will make a million updates in each epoch (1 epoch = 1 pass over the data; 1 step = 1 update). What is the flipside? It is an approximate (rather stochastic) gradient so no guarantee that each step will decrease the loss.</p> <h4>SGD In Action</h4> <p>Let us see this algorithm geometrically when we have a few data points.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/510/1*EM62UF9uHIxgNmJQETYgZw.gif"></figure> <p>If you look closely, you can see that our descent makes many tiny oscillations. Why? Because we are making greedy decisions. Each point is trying to push the parameters in a direction most favorable to it (without being aware of how this affects other points). A parameter update which is locally favorable to one point may harm other points (its almost as if the data points are competing with each other). Can we reduce the oscillations by improving our stochastic estimates of the gradient (currently estimated from just 1 data point at a time)? Yes, let’s look at mini-batch gradient descent.</p> <h3>Mini-Batch Gradient Descent</h3> <p>In the case of mini-batch, instead of making an update with gradients of one data point at a time, we calculate gradients of a batch of data points, of size, say k. Let’s look at the code of MBGD.</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/8c1bfc16dda6de8e308403a00a3d5a97/href" rel="external nofollow noopener" target="_blank">https://medium.com/media/8c1bfc16dda6de8e308403a00a3d5a97/href</a></iframe> <p>Notice that the algorithm updates the parameters after it sees a <em>mini_batch_size</em> number of data points. The stochastic estimates should now be slightly better.</p> <h4>Mini-Batch In Action</h4> <p>Let’s see this algorithm in action when we have <em>k</em>/<em>mini_batch_size</em> = 2.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/470/1*jGzsLBN07fS4ns4kFZmrvQ.gif"></figure> <p>Even with a batch size of <em>k=2,</em> the oscillations have reduced slightly. Why? Because we now have somewhat better estimates of the gradient (analogy: we are now tossing the coin <em>k=2</em> times to estimate P(heads)). The higher the value of k, the more accurate the estimates will be. In practice, typical values of k are 16, 32, 64. Of course, there are still oscillations, and they will always be there as long as we are using an approximate gradient as opposed<br>to the ‘true gradient.’</p> <p>The illustration isn’t clear, and on top of that the <em>k</em> is just 2, so it is hard to see much of a difference. But trust the math, mini-batch helps us get slightly better gradient estimates.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*eoh2R7LXQ2UmRUYdeGxsEA.png"></figure> <h3>SGD With Momentum And NAG</h3> <p>We can have stochastic versions of momentum based gradient descent and Nesterov accelerated gradient descent.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/801/1*30tXOexQixKbDPkRjJicNA.png"></figure> <p>While the stochastic versions of both Momentum (red)and NAG (blue) exhibit oscillations the relative advantage of NAG over Momentum still holds (i.e., NAG takes relatively shorter u-turns). Furthermore, both of them are faster than stochastic gradient descent (after 60 steps, stochastic gradient descent [black - left figure] still exhibits a very high error whereas NAG and momentum are close to convergence).</p> <p>And, of course, we can also have the mini-batch versions of Momentum and NAG but just not in this post.</p> <h3>Conclusion</h3> <p>In this part of the learning parameters series, we looked at variations of the gradient descent algorithm that approximate the gradients updates — Stochastic Gradient Descent and Mini-Batch Gradient Descent. We looked at the key differences between them, python code implementations of both the methods and also illustrated their convergence graphically on a toy problem. We also visualized stochastic versions of Momentum and NAG. In the next post, we will discuss a few useful tips for adjusting the learning rate and momentum related parameters and briefly look at what line search is.</p> <p>Read all about it in the next post of this series at:</p> <ul><li><a href="https://towardsdatascience.com/learning-parameters-part-4-6a18d1d3000b" rel="external nofollow noopener" target="_blank">Learning Parameters, Part 4: Tips For Adjusting Learning Rate &amp; Momentum, Line Search</a></li></ul> <h3>Acknowledgment</h3> <p>A lot of credit goes to <a href="https://www.cse.iitm.ac.in/~miteshk/" rel="external nofollow noopener" target="_blank"><strong>Prof. Mitesh M Khapra</strong></a> and the TAs of <a href="https://www.cse.iitm.ac.in/~miteshk/CS7015.html" rel="external nofollow noopener" target="_blank"><strong>CS7015: Deep Learning</strong></a><strong> </strong>course by IIT Madras for such rich content and creative visualizations. I merely just compiled the provided lecture notes and lecture videos concisely.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/471/1*gRfVnXstuPNmQX_326P9QQ.png"></figure> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ee8558f65dd7" width="1" height="1" alt="">&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/towards-data-science/learning-parameters-part-3-ee8558f65dd7" rel="external nofollow noopener" target="_blank">Learning Parameters, Part 3: Stochastic &amp; Mini-Batch Gradient Descent</a> was originally published in <a href="https://medium.com/towards-data-science" rel="external nofollow noopener" target="_blank">TDS Archive</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p> </body></html>