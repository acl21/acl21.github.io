<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h4><a href="https://medium.com/tag/learning-parameters/latest" rel="external nofollow noopener" target="_blank">Learning Parameters</a></h4> <h3>Learning Parameters, Part 2: Momentum-Based &amp; Nesterov Accelerated Gradient Descent</h3> <h4>Let’s look at two simple, yet very useful variants of gradient descent.</h4> <p>In this post, we look at how the gentle-surface limitation of Gradient Descent can be overcome using the concept of momentum to some extent. Make sure you check out my blog post — <a href="https://towardsdatascience.com/learning-parameters-part-1-eb3e8bb9ffbb" rel="external nofollow noopener" target="_blank">Learning Parameters, Part-1: Gradient Descent</a>, if you are unclear of what this is about. Throughout the blog post, we work with the same toy problem introduced in part-1. You can check out all the posts in the <strong><em>Learning Parameters </em></strong>series by clicking on the kicker tag at the top of this post.</p> <p>In part-1, we saw a clear illustration of a curve where the gradient can be small in gentle regions of the error surface, and this could slow things down. Let’s look at what momentum has to offer overcome this drawback.</p> <blockquote>Citation Note: Most of the content and figures in this blog are directly taken from Lecture 5 of <a href="https://www.cse.iitm.ac.in/~miteshk/CS7015.html" rel="external nofollow noopener" target="_blank">CS7015: Deep Learning</a> course offered by <a href="https://www.cse.iitm.ac.in/~miteshk/" rel="external nofollow noopener" target="_blank">Prof. Mitesh Khapra</a> at IIT-Madras.</blockquote> <h3>Momentum-Based Gradient Descent</h3> <p>The intuition behind MBGD from the mountaineer’s perspective (yes the same trite metaphor we used in part-1) is</p> <blockquote>If I am repeatedly being asked to move in the same direction then I should probably gain some confidence and start taking bigger steps in that direction. Just as a ball gains momentum while rolling down a slope.</blockquote> <h4>The Momentum Update Rule</h4> <p>We accommodate the momentum concept in the gradient update rule as follows:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/461/1*NosPqDgX13dZbvs8E-a7FA.png"></figure> <p>In addition to the current update, we also look at the history of updates. I encourage you to take your time to process the new update rule and try and put it on paper how the <em>update </em>term changes in every step. Or keep reading. Breaking it down we get</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/908/1*ZDBsBanwX5jSFse4m2fxMQ.png"></figure> <p>You can see that the current update is proportional to not just the present gradient but also gradients of previous steps, although their contribution reduces every time step by <strong><em>γ</em></strong>(gamma)<em> </em>times. And that is how we boost the magnitude of the update at gentle regions.</p> <h3>Momentum In Action</h3> <p>We modify the vanilla gradient descent code (shown in Part-1 &amp; also available <a href="https://gist.github.com/akshaychandra21/703ecc8949a01f472d17db3359d56985" rel="external nofollow noopener" target="_blank">here</a>) a little bit as follows:</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/9a502eeae55cdd5efa99837817179ab6/href" rel="external nofollow noopener" target="_blank">https://medium.com/media/9a502eeae55cdd5efa99837817179ab6/href</a></iframe> <p>From now on, we will only work with contour maps. Visualizing things in 3-D can be cumbersome, so contour maps come in as a handy alternative for representing functions with 2-D input and 1-D output. If you are unaware of/uncomfortable with them, please go through <strong>section 5</strong> of my basic stuff blog post — <a href="http://asasasasas" rel="external nofollow noopener" target="_blank"><em>Learning Parameters, Part-0: Basic Stuff</em></a> (there is even a self-test you can take to get better at interpreting them). Let’s see how effective MBGD is using the same toy neural network we introduced in part-1.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/494/1*sbUtb2ySE2DUSBYHhQJRaw.gif"><figcaption>Momentum-Based Gradient Descent. 100 iterations of vanilla gradient descent make the black patch.</figcaption></figure> <p>It works. 100 iterations of vanilla gradient descent make the black patch, and it is evident that even in the regions having gentle slopes, momentum-based gradient descent can take substantial steps because the momentum carries it along.</p> <p>On a critical note, is moving fast always good? Would there be a situation where momentum would cause us to overshoot and run past our goal? Let test the MBGD by changing our input data so that we end up with a different error surface.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/428/1*sVpzlIuRspAhsS7Pnv0pWA.png"></figure> <p>Say this one shown above, where the error is high on either side of the minima valley. Could momentum still work well in such cases or could it be detrimental instead? Let’s find out.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/496/1*t-kykynrtQ0olmFeNgIB0w.gif"><figcaption>100 iterations of vanilla gradient descent make the black patch.</figcaption></figure> <p>We can observe that momentum-based gradient descent oscillates in and out of the minima valley as the momentum carries it out of the valley. This makes us take a lot of U-turns before finally converging. Despite these U-turns, it still converges faster than vanilla gradient descent. After 100 iterations momentum-based method has reached an error of 0.00001 whereas vanilla gradient descent is still stuck at an error of 0.36.</p> <p>Can we do something to reduce the oscillations/U-turns? Yes, Nesterov Accelerated Gradient Descent helps us do just that.</p> <h3>Nesterov Accelerated Gradient Descent</h3> <p>The intuition behind NAG can be put into a single phrase:</p> <blockquote>Look ahead before you leap!</blockquote> <figure><img alt="" src="https://cdn-images-1.medium.com/max/693/1*zjEZHrbahO4RSA7HHgsL6w.png"></figure> <h4>The NAG Update Rule</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/508/1*ewQ9mtcJW00Dgp0ZJFNKmg.png"></figure> <p>But why would looking ahead help us in avoiding overshoots? I urge you to pause and ponder. If it’s not clear, I am sure it will be clear in the next few minutes. Take a look at this figure for a moment.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*6MEi74EMyPERHlAX-x2Slw.png"><figcaption>A toy illustration.</figcaption></figure> <p>In figure (a), update 1 is positive i.e., the gradient is negative because as <strong><em>w_0</em></strong> increases <strong><em>L</em></strong> decreases. Even update 2 is positive as well and you can see that the update is slightly larger than update 1, thanks to momentum. By now, you should be convinced that update 3 will be bigger than both update 1 and 2 simply because of momentum and the positive update history. Update 4 is where things get interesting. In vanilla momentum case, due to the positive history, the update overshoots and the descent recovers by doing negative updates.</p> <p>But in NAG’s case, every update happens in two steps — first, a partial update, where we get to the <em>look_ahead</em> point and then the final update (see the NAG update rule), see figure (b). First 3 updates of NAG are pretty similar to the momentum-based method as both the updates (partial and final) are positive in those cases. But the real difference becomes apparent during update 4. As usual, each update happens in two stages, the partial update (4a) is positive, but the final update (4b) would be negative as the calculated gradient at <strong><em>w_lookahead </em></strong>would be negative (convince yourself by observing the graph). This negative final update slightly reduces the overall magnitude of the update, still resulting in an overshoot but a smaller one when compared to the vanilla momentum-based gradient descent. And that my friend, is how NAG helps us in reducing the overshoots, i.e. making us take shorter U-turns.</p> <h3>NAG In Action</h3> <p>By updating the momentum code slightly to do both the partial update and the full update, we get the code for NAG.</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/4dbc0f1e6f9e762def97e1f2d0e43376/href" rel="external nofollow noopener" target="_blank">https://medium.com/media/4dbc0f1e6f9e762def97e1f2d0e43376/href</a></iframe> <p>Let’s compare the convergence of momentum-based method with NAG using the same toy example and the same error surface we used while illustrating the momentum-based method.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/496/1*_Q1plMUkXfLPTRCCTRg36g.gif"><figcaption>You can see that (I hope you can) NAG (blue) is taking smaller U-turns compared to vanilla momentum (red).</figcaption></figure> <p>NAG is certainly making smaller oscillations/taking shorter U-turns even when approaching the minima valleys on the error surface. Looking ahead helps NAG in correcting its course quicker than momentum-based gradient descent. Hence the oscillations are smaller and the chances of escaping the minima valley also smaller. Earlier, we proved that looking back helps <em>*ahem momentum ahem*</em> and we now proved that looking ahead also helps.</p> <h3>Conclusion</h3> <p>In this blog post, we looked at two simple, yet hybrid versions of gradient descent that help us converge faster — <em>Momentum-Based Gradient Descent</em> and <em>Nesterov Accelerated Gradient Descent (NAG) </em>and also discussed why and where NAG beats vanilla momentum-based method. We looked at the nuances in their update rules, python code implementations of the methods and also illustrated their convergence graphically on a toy example. In the next post, we will digress a little bit and talk about stochastic versions of these algorithms.</p> <p>Read all about it in the next post of this series at:</p> <ul><li><a href="https://towardsdatascience.com/ee8558f65dd7" rel="external nofollow noopener" target="_blank">Learning Parameters, Part 3: Stochastic &amp; Mini-Batch Gradient Descent</a></li></ul> <h3>Acknowledgment</h3> <p>A lot of credit goes to <a href="https://www.cse.iitm.ac.in/~miteshk/" rel="external nofollow noopener" target="_blank"><strong>Prof. Mitesh M Khapra</strong></a> and the TAs of <a href="https://www.cse.iitm.ac.in/~miteshk/CS7015.html" rel="external nofollow noopener" target="_blank"><strong>CS7015: Deep Learning</strong></a><strong> </strong>course by IIT Madras for such rich content and creative visualizations. I merely just compiled the provided lecture notes and lecture videos concisely.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a190bef2d12" width="1" height="1" alt="">&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/data-science/learning-parameters-part-2-a190bef2d12" rel="external nofollow noopener" target="_blank">Learning Parameters, Part 2: Momentum-Based And Nesterov Accelerated Gradient Descent</a> was originally published in <a href="https://medium.com/data-science" rel="external nofollow noopener" target="_blank">TDS Archive</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p> </body></html>