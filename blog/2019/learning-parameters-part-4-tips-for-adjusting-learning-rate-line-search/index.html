<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h4><a href="https://medium.com/tag/learning-parameters/latest" rel="external nofollow noopener" target="_blank">Learning Parameters</a></h4> <h3>Learning Parameters, Part 4: Tips For Adjusting Learning Rate, Line Search</h3> <h4>Before moving on to advanced optimization algorithms let us revisit the problem of learning rate in gradient descent.</h4> <p>In <a href="https://towardsdatascience.com/learning-parameters-part-3-ee8558f65dd7" rel="external nofollow noopener" target="_blank">part 3</a>, we looked at stochastics and mini-batch versions of the optimizers. In this post, we will look at some commonly followed heuristics on how to tune the learning rate, etc. If you are not interested in these heuristics, feel free to skip to <a href="https://towardsdatascience.com/learning-parameters-part-5-65a2f3583f7d" rel="external nofollow noopener" target="_blank">part 5</a> of the <a href="https://medium.com/tag/learning-parameters/latest" rel="external nofollow noopener" target="_blank">Learning Parameters</a> series.</p> <blockquote>Citation Note: Most of the content and figures in this blog are directly taken from Lecture 5 of <a href="https://www.cse.iitm.ac.in/~miteshk/CS7015.html" rel="external nofollow noopener" target="_blank">CS7015: Deep Learning</a> course offered by <a href="https://www.cse.iitm.ac.in/~miteshk/" rel="external nofollow noopener" target="_blank">Prof. Mitesh Khapra</a> at IIT-Madras.</blockquote> <p>One could argue that we could have solved the problem of navigating gentle slopes by setting the learning rate high (i.e., blow up the small gradient by multiplying it with a large learning rate <strong><em>η</em></strong>). This seemingly trivial idea does sometimes work at gentle slopes of the error function, but it fails to work when the error surface is flat. Here’s an example:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/468/1*umP0flU2tdvAAfW_qfli_g.gif"></figure> <p>Clearly, on the regions which have a steep slope, the already large gradient blows up further and the large learning rate sort of helps the cause but as soon as the error surface flattens, it doesn’t help a lot. It would be safe to assume that it is always good to have a learning rate which could adjust to the gradient and we will see a few such algorithms in the next post (part 5) in the Learning Parameters series.</p> <h3>Some Useful Tips</h3> <h4>Tips for Initial Learning Rate</h4> <ul> <li>Tune learning rate. Try different values on a log scale: 0.0001, 0.001, 0.01, 0.1, 1.0.</li> <li>Run a few epochs with each of these and figure out a learning rate which works best.</li> <li>Now do a finer search around this value. For example, if the best learning rate was 0.1 then now try some values around it: 0.05, 0.2, 0.3.</li> <li>Disclaimer: these are just heuristics, no clear winner strategy.</li> </ul> <h4>Tips for Annealing Learning Rate</h4> <p><strong>Step Decay</strong></p> <ul> <li>Halve the learning rate after every 5 epochs</li> <li>Halve the learning rate after an epoch if the validation error is more than what it was at the end of the previous epoch</li> </ul> <p><strong>Exponential Decay</strong></p> <ul><li> <em>η = η₀⁻ᵏᵗ</em>, where <em>η₀</em><strong><em> </em></strong>and <em>k</em> are hyperparameters and t is the step number</li></ul> <p><strong>1/t Decay</strong></p> <ul><li> <em>η = (η₀)/(1+kt), </em>where <em>η₀</em><strong><em> </em></strong>and <em>k</em> are hyperparameters and t is the step number.</li></ul> <h4>Tips for Momentum</h4> <p>The following schedule was suggested by Sutskever <em>et al.</em>, 2013</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/467/1*0LV_yZM5e516h1YFHaFXDA.png"></figure> <p>where, <em>γ_max</em> was chosen from {0.999, 0.995, 0.99, 0.9, 0}.</p> <h3>Line Search</h3> <p>In practice, often a line search is done to find a relatively better value of <em>η</em>. In line search, we update <em>w</em> using different learning rates (<em>η</em>) and check the updated model’s error in every iteration. Ultimately, we retain that updated value of <em>w</em> which gives the lowest loss. Take a look at the code:</p> <p>Essentially at each step, we are trying to use the best <em>η</em> value from the available choices. This is obviously not the best idea. We are doing many more computations in each step but that’s the trade-off for finding the best learning rate. Today, there are cooler ways to do this.</p> <h4>Line Search in Action</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/468/1*8lDYTCFKGAUHcsJpbzdGsQ.gif"></figure> <p>Clearly, convergence is faster than vanilla gradient descent (see <a href="https://towardsdatascience.com/learning-parameters-part-1-eb3e8bb9ffbb?source=your_stories_page---------------------------" rel="external nofollow noopener" target="_blank">part 1</a>). We see some oscillations but notice that these oscillations are quite different from what we see in momentum and NAG (see <a href="https://towardsdatascience.com/learning-parameters-part-2-a190bef2d12" rel="external nofollow noopener" target="_blank">part 2</a>).</p> <p><strong>Note:</strong> Leslie N. Smith in his 2015 paper, <a href="https://arxiv.org/pdf/1506.01186.pdf" rel="external nofollow noopener" target="_blank"><em>Cyclical Learning Rates for Training Neural Networks</em></a><em> </em>proposed a smarter way than line search. I refer the reader to <a href="https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0" rel="external nofollow noopener" target="_blank">this medium post</a> by <a href="https://towardsdatascience.com/@surmenok" rel="external nofollow noopener" target="_blank">Pavel Surmenok</a> to read more about it.</p> <h3>Conclusion</h3> <p>In this part of the learning parameters series, we looked at some heuristic that can help us tune the learning rate and momentum for better training. We also looked at Line Search, a once-popular method to finding the best learning rate at every step of the gradient update. In the next (final) part of the learning parameters series, we will closely look at gradient descent with adaptive learning rate, specifically the following optimizers — AdaGrad, RMSProp, and Adam.</p> <p>You can find the next part here:</p> <ul><li><a href="https://towardsdatascience.com/learning-parameters-part-5-65a2f3583f7d" rel="external nofollow noopener" target="_blank">Learning Parameters, Part 5: AdaGrad, RMSProp, and Adam</a></li></ul> <h3>Acknowledgment</h3> <p>A lot of credit goes to <a href="https://www.cse.iitm.ac.in/~miteshk/" rel="external nofollow noopener" target="_blank"><strong>Prof. Mitesh M Khapra</strong></a> and the TAs of <a href="https://www.cse.iitm.ac.in/~miteshk/CS7015.html" rel="external nofollow noopener" target="_blank"><strong>CS7015: Deep Learning</strong></a><strong> </strong>course by IIT Madras for such rich content and creative visualizations. I merely just compiled the provided lecture notes and lecture videos concisely.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*fRGHjjqjl5P6IArtZpx_Hg.jpeg"></figure> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=6a18d1d3000b" width="1" height="1" alt="">&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/data-science/learning-parameters-part-4-6a18d1d3000b" rel="external nofollow noopener" target="_blank">Learning Parameters Part 4: Tips For Adjusting Learning Rate, Line Search</a> was originally published in <a href="https://medium.com/data-science" rel="external nofollow noopener" target="_blank">TDS Archive</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p> </body></html>