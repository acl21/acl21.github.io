<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h3>Mouse Cursor Control Using Facial Movements — An HCI Application</h3> <p>This HCI (Human-Computer Interaction) application in Python(3.6) will allow you to control your mouse cursor with your facial movements, works with just your regular webcam. Its hands-free, no wearable hardware or sensors needed.</p> <p>Special thanks to <strong>Adrian Rosebrock</strong> for his amazing blog posts [2] [3], code snippets and his <a href="https://github.com/jrosebr1/imutils" rel="external nofollow noopener" target="_blank">imutils</a> library [7] that played an important role in making this idea of mine a reality.</p> <h3>Working Example</h3> <iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FL2XUKeLD6N8%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DL2XUKeLD6N8&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FL2XUKeLD6N8%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/e2b36aa0f951ed26358c51a43691a8a4/href" rel="external nofollow noopener" target="_blank">https://medium.com/media/e2b36aa0f951ed26358c51a43691a8a4/href</a></iframe> <h3>Usage</h3> <p>Now, I definitely understand that these facial movements could be a little bit weird to do, especially when you are around people. Being a patient of <a href="https://www.healthline.com/health/benign-positional-vertigo" rel="external nofollow noopener" target="_blank">benign-positional-vertigo</a>, I hate doing some of these actions myself. But I hope to make them easier and less weird over time. Feel free to suggest some public friendly actions that I can incorporate in the project.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*OqOtL0Sffvqk86jG5sGAFw.jpeg"></figure> <h3>Code</h3> <p>You can find the code files here:</p> <p><a href="https://github.com/acl21/Mouse_Cursor_Control_Handsfree" rel="external nofollow noopener" target="_blank">acl21/Mouse_Cursor_Control_Handsfree</a></p> <h4>Libraries Used</h4> <ul> <li>Numpy — 1.13.3</li> <li>OpenCV — 3.2.0</li> <li>PyAutoGUI — 0.9.36</li> <li>Dlib — 19.4.0</li> <li>Imutils — 0.4.6</li> </ul> <p>Execution steps are mentioned in the README.md of the repo. Feel free to raise an issue in case of any errors.</p> <h3>How It Works</h3> <p>This project is deeply centered around predicting the facial landmarks of a given face. We can accomplish a lot of things using these landmarks. From detecting eye-blinks [3] in a video to predicting emotions of the subject. The applications, outcomes, and possibilities of facial landmarks are immense and intriguing.</p> <p><a href="http://dlib.net/" rel="external nofollow noopener" target="_blank">Dlib</a>’s prebuilt model, which is essentially an implementation of [4], not only does a fast face-detection but also allows us to accurately predict 68 2D facial landmarks. Very handy.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*EZ9o_UGKR6Z-F_Ea2jmyGg.jpeg"></figure> <p>Using these predicted landmarks of the face, we can build appropriate features that will further allow us to detect certain actions, like using the eye-aspect-ratio (more on this below) to detect a blink or a wink, using the mouth-aspect-ratio to detect a yawn etc or maybe even a pout. In this project, these actions are programmed as triggers to control the mouse cursor. <a href="http://pyautogui.readthedocs.io" rel="external nofollow noopener" target="_blank">PyAutoGUI</a> library was used to move the cursor around.</p> <h4>Eye-Aspect-Ratio (EAR)</h4> <p>You will see that Eye-Aspect-Ratio [1] is the simplest and the most elegant feature that takes good advantage of the facial landmarks. EAR helps us in detecting blinks [3] and winks etc.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/772/1*6Ix1R90EmXixWYd5MGSJdQ.png"></figure> <p>You can see that the EAR value drops whenever the eye closes. We can train a simple classifier to detect the drop. However, a normal <em>if</em> condition works just fine. Something like this:</p> <pre>if EAR &lt;= SOME_THRESHOLD:<br>      EYE_STATUS = 'CLOSE'</pre> <h4>Mouth-Aspect-Ratio (MAR)</h4> <p>Highly inspired by the EAR feature, I tweaked the formula a little bit to get a metric that can detect opened/closed mouth. Unoriginal but it works.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/754/1*LC6YYNhB3aIzMTMVMY5Pdg.jpeg"></figure> <p>Similar to EAR, MAR value goes up when the mouth opens. Similar intuitions hold true for this metric as well.</p> <h3>Prebuilt Model Details</h3> <p>The model offers two important functions. A detector to detect the face and a predictor to predict the landmarks. The face detector used is made using the classic Histogram of Oriented Gradients (HOG) feature combined with a linear classifier, an image pyramid, and sliding window detection scheme.</p> <p>The facial landmarks estimator was created by using Dlib’s implementation of the paper: <a href="https://www.semanticscholar.org/paper/One-millisecond-face-alignment-with-an-ensemble-of-Kazemi-Sullivan/1824b1ccace464ba275ccc86619feaa89018c0ad" rel="external nofollow noopener" target="_blank"><em>One Millisecond Face Alignment with an Ensemble of Regression Trees by Vahid Kazemi and Josephine Sullivan</em></a>, CVPR 2014. And was trained on the iBUG 300-W face landmark dataset: C. Sagonas, E. Antonakos, G, Tzimiropoulos, S. Zafeiriou, M. Pantic. 300 faces In-the-wild challenge: Database and results. <a href="https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/" rel="external nofollow noopener" target="_blank"><em>Image and Vision Computing (IMAVIS), Special Issue on Facial Landmark Localisation “In-The-Wild”. 2016</em></a>.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/872/1*rmy_Dtu2Dd5h5QbvbKbpag.png"></figure> <p>You can get the trained model file from <a href="http://dlib.net/files," rel="external nofollow noopener" target="_blank">http://dlib.net/files,</a> click on <strong>shape_predictor_68_face_landmarks.dat.bz2</strong>. The model, .dat file has to be in the project folder.</p> <p><strong>Note:</strong> The license for the iBUG 300-W dataset excludes commercial use. So you should contact Imperial College London to find out if it’s OK for you to use this model file in a commercial product.</p> <h3>References</h3> <p><strong>[1]</strong>. Tereza Soukupova´ and Jan Cˇ ech. <a href="https://vision.fe.uni-lj.si/cvww2016/proceedings/papers/05.pdf" rel="external nofollow noopener" target="_blank"><em>Real-Time Eye Blink Detection using Facial Landmarks</em></a>. In 21st Computer Vision Winter Workshop, February 2016.<br><strong>[2]</strong>. Adrian Rosebrock. <a href="https://www.pyimagesearch.com/2017/04/10/detect-eyes-nose-lips-jaw-dlib-opencv-python/" rel="external nofollow noopener" target="_blank"><em>Detect eyes, nose, lips, and jaw with dlib, OpenCV, and Python</em></a>. <br><strong>[3]</strong>. Adrian Rosebrock. <a href="https://www.pyimagesearch.com/2017/04/24/eye-blink-detection-opencv-python-dlib/" rel="external nofollow noopener" target="_blank"><em>Eye blink detection with OpenCV, Python, and dlib</em></a>.<br><strong>[4]</strong>. Vahid Kazemi, Josephine Sullivan. <a href="https://ieeexplore.ieee.org/document/6909637" rel="external nofollow noopener" target="_blank"><em>One millisecond face alignment with an ensemble of regression trees</em></a>. In CVPR, 2014.<br><strong>[5]</strong>. S. Zafeiriou, G. Tzimiropoulos, and M. Pantic. <a href="http://ibug.doc.ic.ac.uk/resources/300-VW/.3" rel="external nofollow noopener" target="_blank"><em>The 300 videos in the wild (300-VW) facial landmark tracking in-the-wild challenge</em></a>. In ICCV Workshop, 2015. <br><strong>[6]</strong>. C. Sagonas, G. Tzimiropoulos, S. Zafeiriou, M. Pantic. <a href="https://ibug.doc.ic.ac.uk/media/uploads/documents/sagonas_iccv_2013_300_w.pdf" rel="external nofollow noopener" target="_blank"><em>300 Faces in-the-Wild Challenge: The first facial landmark localization Challenge</em></a>. Proceedings of IEEE Int’l Conf. on Computer Vision (ICCV-W), 300 Faces in-the-Wild Challenge (300-W). Sydney, Australia, December 2013<br><strong>[7]</strong>. Adrian Rosebrock. <em>Imutils</em>. <a href="https://github.com/jrosebr1/imutils" rel="external nofollow noopener" target="_blank">https://github.com/jrosebr1/imutils</a>.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*GENBf59JzMt8oSNmH75mEA.jpeg"><figcaption>Photo by <a href="https://unsplash.com/photos/28XgB4jULqg?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="external nofollow noopener" target="_blank">Orysya Dibrova</a> on <a href="https://unsplash.com/search/photos/computer-mouse-handsfree?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="external nofollow noopener" target="_blank">Unsplash</a></figcaption></figure> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c16b0494a971" width="1" height="1" alt="">&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/towards-data-science/mouse-control-facial-movements-hci-app-c16b0494a971" rel="external nofollow noopener" target="_blank">Mouse Cursor Control Using Facial Movements — An HCI Application</a> was originally published in <a href="https://medium.com/towards-data-science" rel="external nofollow noopener" target="_blank">TDS Archive</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p> </body></html>