<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>This post will discuss the famous <em>Perceptron Learning Algorithm,</em> originally proposed by <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&amp;rep=rep1&amp;type=pdf" rel="external nofollow noopener" target="_blank">Frank Rosenblatt</a> in 1943, later refined and carefully analyzed by <a href="http://science.sciencemag.org/content/165/3895/780" rel="external nofollow noopener" target="_blank">Minsky and Papert</a> in 1969. This is a follow-up post of my previous posts on the <a href="https://towardsdatascience.com/mcculloch-pitts-model-5fdf65ac5dd1" rel="external nofollow noopener" target="_blank">McCulloch-Pitts neuron</a> model and the <a href="https://towardsdatascience.com/4d8c70d5cc8d" rel="external nofollow noopener" target="_blank">Perceptron</a> model.</p> <p><em>Citation Note: The concept, the content, and the structure of this article were entirely based on Prof. </em><a href="https://www.cse.iitm.ac.in/~miteshk/" rel="external nofollow noopener" target="_blank"><strong><em>Mitesh Khapra</em></strong></a><em>’s</em><strong><em> </em></strong><em>lectures slides and videos of the </em><a href="https://www.cse.iitm.ac.in/~miteshk/CS7015.html" rel="external nofollow noopener" target="_blank"><em>CS7015: Deep Learning</em></a><em> course taught at IIT Madras.</em></p> <h3>Perceptron</h3> <p>You can just go through my previous post on the perceptron model (linked above) but I will assume that you won’t. So here goes, a perceptron is not the Sigmoid neuron we use in ANNs or any deep learning networks today.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/870/1*Fyapb-JRFJ-VtnLYLLXCwg.png"></figure> <p>The perceptron model is a more general computational model than McCulloch-Pitts neuron. It takes an input, aggregates it (weighted sum) and returns 1 only if the aggregated sum is more than some threshold else returns 0. Rewriting the threshold as shown above and making it a constant input with a variable weight, we would end up with something like the following:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/870/1*gKFs7YU44vJFiS2rF3-bpg.png"></figure> <p>A single perceptron can only be used to implement <strong>linearly separable</strong> functions. It takes both real and boolean inputs and associates a set of <strong>weights </strong>to them, along with a <strong>bias </strong>(the threshold thing I mentioned above). We learn the weights, we get the function. Let's use a perceptron to learn an OR function.</p> <h4>OR Function Using A Perceptron</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/910/1*C5LeL8JDfoGbkUg0cu1M-w.png"></figure> <p>What’s going on above is that we defined a few conditions (the weighted sum has to be more than or equal to 0 when the output is 1) based on the OR function output for various sets of inputs, we solved for weights based on those conditions and we got a line that perfectly separates positive inputs from those of negative.</p> <p>Doesn’t make any sense? Maybe now is the time you go through that <a href="https://towardsdatascience.com/4d8c70d5cc8d" rel="external nofollow noopener" target="_blank">post</a> I was talking about. Minsky and Papert also proposed a more principled way of learning these weights using a set of examples (data). Mind you that this is NOT a Sigmoid neuron and we’re not going to do any Gradient Descent.</p> <h3><strong>Warming Up — Basics Of Linear Algebra</strong></h3> <h4>Vector</h4> <p>A vector can be defined in more than one way. For a physicist, a vector is anything that sits anywhere in space, has a magnitude and a direction. For a CS guy, a vector is just a data structure used to store some data — integers, strings etc. For this tutorial, I would like you to imagine a vector the Mathematician way, where a vector is an arrow spanning in space with its tail at the origin. This is not the best mathematical way to describe a vector but as long as you get the intuition, you’re good to go.</p> <p><em>Note: I have borrowed the following screenshots from </em><a href="http://www.3blue1brown.com/" rel="external nofollow noopener" target="_blank"><em>3Blue1Brown</em></a><em>’s video on </em><a href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab" rel="external nofollow noopener" target="_blank"><em>Vectors</em></a><em>. If you don’t know him already, please check his series on </em><a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab" rel="external nofollow noopener" target="_blank"><em>Linear Algebra</em></a><em> and </em><a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr" rel="external nofollow noopener" target="_blank"><em>Calculus</em></a><em>. He is just out of this world when it comes to visualizing Math.</em></p> <h4><strong>Vector Representations</strong></h4> <p>A 2-dimensional vector can be represented on a 2D plane as follows:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/599/1*WfqYmJ_bWDIdGvTQOjFZ2A.png"><figcaption><em>Source: </em><a href="http://www.3blue1brown.com/" rel="external nofollow noopener" target="_blank"><em>3Blue1Brown</em></a><em>’s video on </em><a href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab" rel="external nofollow noopener" target="_blank"><em>Vectors</em></a></figcaption></figure> <p>Carrying the idea forward to 3 dimensions, we get an arrow in 3D space as follows:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/631/1*KFSKU4q6InY1PLprWfy1ig.png"><figcaption><em>Source: </em><a href="http://www.3blue1brown.com/" rel="external nofollow noopener" target="_blank"><em>3Blue1Brown</em></a><em>’s video on </em><a href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab" rel="external nofollow noopener" target="_blank"><em>Vectors</em></a></figcaption></figure> <h4>Dot Product Of Two Vectors</h4> <p>At the cost of making this tutorial even more boring than it already is, let's look at what a dot product is. Imagine you have two vectors oh size <strong>n+1</strong>, <strong>w</strong> and <strong>x</strong>, the dot product of these vectors (<strong><em>w.x</em></strong>) could be computed as follows:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/334/1*0zI1zKIOakgNuPMIg__UJg.png"><figcaption>The transpose is just to write it in a matrix multiplication form.</figcaption></figure> <p>Here, <strong>w</strong> and <strong>x</strong> are just two lonely arrows in an <strong>n+1 dimensional</strong> space (and intuitively, their dot product quantifies how much one vector is going in the direction of the other). So technically, the perceptron was only computing a lame dot product (before checking if it's greater or lesser than 0). The decision boundary line which a perceptron gives out that separates positive examples from the negative ones is really just <strong>w . x </strong>=<strong> </strong>0.</p> <h4>Angle Between Two Vectors</h4> <p>Now the same old dot product can be computed differently if only you knew the angle between the vectors and their individual magnitudes. Here’s how:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/302/1*-VwGx_ttwoyzYFKAIdX0SQ.png"></figure> <p>The other way around, you can get the angle between two vectors, if only you knew the vectors, given you know how to calculate vector magnitudes and their vanilla dot product.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/514/1*Gltd0rXq2ne62BscH6Vf2A.png"></figure> <p>When I say that the cosine of the angle between <strong>w</strong> and <strong>x</strong> is 0, what do you see? I see arrow <strong>w</strong> being perpendicular to arrow <strong>x</strong> in<strong> </strong>an n+1 dimensional space (in 2-dimensional space to be honest). So basically, when the dot product of two vectors is 0, they are perpendicular to each other.</p> <h4>Setting Up The Problem</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/942/1*NZ6814J6M0xB-P896pylrA.png"></figure> <p>We are going to use a perceptron to estimate if I will be watching a movie based on historical data with the above-mentioned inputs. The data has positive and negative examples, positive being the movies I watched i.e., 1. Based on the data, we are going to learn the weights using the perceptron learning algorithm. For visual simplicity, we will only assume two-dimensional input.</p> <h3>Perceptron Learning Algorithm</h3> <p>Our goal is to find the <strong>w</strong> vector that can perfectly classify positive inputs and negative inputs in our data. I will get straight to the algorithm. Here goes:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/516/1*PbJBdf-WxR0Dd0xHvEoh4A.png"></figure> <p>We initialize <strong>w </strong>with some random vector. We then iterate over all the examples in the data, (<em>P</em> U <em>N</em>) both positive and negative examples. Now if an input <strong>x</strong> belongs to <em>P</em>, ideally what should the dot product <strong>w.x</strong> be? I’d say greater than or equal to 0 because that’s the only thing what our perceptron wants at the end of the day so let's give it that. And if <strong>x </strong>belongs to <em>N</em>, the dot product MUST be less than 0. So if you look at the if conditions in the while loop:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/359/1*iXjNpsP40-UYvwxkfQwUhw.png"></figure> <p><strong>Case 1:</strong> When <strong>x</strong> belongs to <em>P </em>and its dot product <strong>w.x</strong> &lt; 0 <br><strong>Case 2:</strong> When <strong>x</strong> belongs to <em>N </em>and its dot product <strong>w.x</strong> ≥ 0</p> <p>Only for these cases, we are updating our randomly initialized <strong>w</strong>. Otherwise, we don’t touch <strong>w</strong> at all because Case 1 and Case 2 are violating the very rule of a perceptron. So we are adding <strong>x</strong> to <strong>w </strong>(ahem vector addition ahem) in Case 1 and subtracting <strong>x </strong>from <strong>w</strong> in Case 2.</p> <h4>Why Would The Specified Update Rule Work?</h4> <p>But why would this work? If you get it already why this would work, you’ve got the entire gist of my post and you can now move on with your life, thanks for reading, bye. But if you are not sure why these seemingly arbitrary operations of <strong>x </strong>and <strong>w</strong> would help you learn that perfect <strong>w</strong> that can perfectly classify <em>P </em>and <em>N</em>, stick with me.</p> <p>We have already established that when <strong>x</strong> belongs to <em>P</em>, we want <strong>w.x</strong> &gt; 0, basic perceptron rule. What we also mean by that is that when <strong>x</strong> belongs to <em>P</em>, the angle between <strong>w</strong> and <strong>x</strong> should be _____ than 90 degrees. Fill in the blank.</p> <p>Answer: The angle between <strong>w</strong> and <strong>x</strong> should be less than 90 because the cosine of the angle is proportional to the dot product.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/533/1*AjFrPxeZEot-apPCAC3mSQ.png"></figure> <p>So whatever the <strong>w</strong> vector may be, as long as it makes an angle less than 90 degrees with the positive example data vectors (<strong>x</strong> E <em>P</em>) and an angle more than 90 degrees with the negative example data vectors (<strong>x </strong>E <em>N</em>), we are cool. So ideally, it should look something like this:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/386/1*D09EzbR-sGbX-qv2jcEPhw.png"><figcaption>x_0 is always 1 so we ignore it for now.</figcaption></figure> <p>So we now strongly believe that the angle between <strong>w</strong> and <strong>x</strong> should be less than 90 when <strong>x</strong> belongs to <em>P</em> class and the angle between them should be more than 90 when <strong>x</strong> belongs to <em>N</em> class. Pause and convince yourself that the above statements are true and you indeed believe them. Here’s why the update works:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Ny1n6xH8g2JR2XVhcGRVvA.png"><figcaption>Now this is slightly inaccurate but it is okay to get the intuition.</figcaption></figure> <p>So when we are adding <strong>x</strong> to <strong>w</strong>, which we do when x belongs to P and<strong> w.x</strong> &lt; 0 (Case 1), we are essentially <strong>increasing the <em>cos(alpha)</em></strong> value, which means, we are <strong>decreasing the <em>alpha</em> value</strong>, the angle between <strong>w </strong>and <strong>x</strong>, <strong>which is what we desire</strong>. And the similar intuition works for the case when <strong>x</strong> belongs to <em>N</em> and <strong>w.x</strong> ≥ 0 (Case 2).</p> <p>Here’s a toy simulation of how we might up end up learning <strong>w</strong> that makes an angle less than 90 for positive examples and more than 90 for negative examples.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/394/1*Nnb6IQW1qAkXi0LG6CTEGQ.gif"><figcaption>We start with a random vector <strong>w</strong>.</figcaption></figure> <h3>Proof Of Convergence</h3> <p>Now, there is no reason for you to believe that this will definitely converge for all kinds of datasets. It seems like there might be a case where the <strong>w</strong> keeps on moving around and never converges. But people have proved it that this algorithm converges. I am attaching the proof, by Prof. Michael Collins of Columbia University — <a href="http://www.cs.columbia.edu/~mcollins/courses/6998-2012/notes/perc.converge.pdf" rel="external nofollow noopener" target="_blank">find the paper here</a>.</p> <h3>Conclusion</h3> <p>In this post, we quickly looked at what a perceptron is. We then warmed up with a few basics of linear algebra. We then looked at the <em>Perceptron Learning Algorithm </em>and then went on to visualize why it works i.e., how the appropriate weights are learned.</p> <p>Thank you for reading this post.<br>Live and let live!<br>A</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*x2gUD6BhNjkF7ac4UEwg5Q.jpeg"><figcaption>Photo by <a href="https://unsplash.com/photos/5mZ_M06Fc9g?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="external nofollow noopener" target="_blank">Roman Mager</a> on <a href="https://unsplash.com/search/photos/math?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="external nofollow noopener" target="_blank">Unsplash</a></figcaption></figure> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d5db0deab975" width="1" height="1" alt="">&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/data-science/perceptron-learning-algorithm-d5db0deab975" rel="external nofollow noopener" target="_blank">Perceptron Learning Algorithm: A Graphical Explanation Of Why It Works</a> was originally published in <a href="https://medium.com/data-science" rel="external nofollow noopener" target="_blank">TDS Archive</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p> </body></html>