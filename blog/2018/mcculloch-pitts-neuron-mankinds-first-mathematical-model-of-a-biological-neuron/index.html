<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>It is very well known that the most fundamental unit of deep neural networks is called an <em>artificial neuron/perceptron</em>. But the very first step towards the <em>perceptron </em>we use today was taken in 1943 by McCulloch and Pitts, by mimicking the functionality of a biological neuron.</p> <p><em>Note: The concept, the content, and the structure of this article were directly taken from the awesome lectures and the material offered by Prof. </em><a href="https://www.cse.iitm.ac.in/~miteshk/" rel="external nofollow noopener" target="_blank"><strong><em>Mitesh M. Khapra</em></strong></a><em> on </em><a href="http://nptel.ac.in" rel="external nofollow noopener" target="_blank"><em>NPTEL</em></a><em>’s </em><a href="https://onlinecourses.nptel.ac.in/noc18_cs41/preview" rel="external nofollow noopener" target="_blank"><em>Deep Learning</em></a><em> course. Check it out!</em></p> <h3>Biological Neurons: An Overly Simplified Illustration</h3> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*1Oh53dNdPITVnoOVGCCUFA.png"><figcaption><strong>A Biological Neuron </strong>— <a href="https://en.wikipedia.org/wiki/Neuron" rel="external nofollow noopener" target="_blank">Wikipedia</a></figcaption></figure> <p><strong>Dendrite</strong>: Receives signals from other neurons</p> <p><strong>Soma</strong>: Processes the information</p> <p><strong>Axon</strong>: Transmits the output of this neuron</p> <p><strong>Synapse</strong>: Point of connection to other neurons</p> <p>Basically, a neuron takes an input signal (dendrite), processes it like the CPU (soma), passes the output through a cable like structure to other connected neurons (axon to synapse to other neuron’s dendrite). Now, this might be biologically inaccurate as there is a lot more going on out there but on a higher level, this is what is going on with a neuron in our brain — takes an input, processes it, throws out an output.</p> <p>Our sense organs interact with the outer world and send the visual and sound information to the neurons. Let's say you are watching Friends. Now the information your brain receives is taken in by the “laugh or not” set of neurons that will help you make a decision on whether to laugh or not. Each neuron gets fired/activated only when its respective criteria (more on this later) is met like shown below.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*HzV09JNYejMnu7AeyUw4oA.png"><figcaption>Not real.</figcaption></figure> <p>Of course, this is not entirely true. In reality, it is not just a couple of neurons which would do the decision making. There is a massively parallel interconnected network of 10¹¹ neurons (100 billion) in our brain and their connections are not as simple as I showed you above. It might look something like this:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*tKCHQq4n3PBGMdT1-o2VlA.png"><figcaption>Still not real but closer.</figcaption></figure> <p>Now the sense organs pass the information to the first/lowest layer of neurons to process it. And the output of the processes is passed on to the next layers in a hierarchical manner, some of the neurons will fire and some won’t and this process goes on until it results in a final response — in this case, laughter.</p> <p>This massively parallel network also ensures that there is a division of work. Each neuron only fires when its intended criteria is met i.e., a neuron may perform a certain role to a certain stimulus, as shown below.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FlSrwIA_SkLQiP2GQA958A.png"><figcaption>Division of work</figcaption></figure> <p>It is believed that neurons are arranged in a hierarchical fashion (however, many credible alternatives with experimental support are proposed by the scientists) and each layer has its own role and responsibility. To detect a face, the brain could be relying on the entire network and not on a single layer.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/510/1*YlMl1P-K6gJZ1LbuezXvVw.png"><figcaption>Sample illustration of hierarchical processing. <strong>Credits: </strong>Mitesh M. Khapra’s lecture slides</figcaption></figure> <p>Now that we have established how a biological neuron works, lets look at what McCulloch and Pitts had to offer.</p> <p><em>Note: My understanding of how the brain works is very very very limited. The above illustrations are overly simplified.</em></p> <h3>McCulloch-Pitts Neuron</h3> <p>The first computational model of a neuron was proposed by Warren MuCulloch (neuroscientist) and Walter Pitts (logician) in 1943.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/369/1*fDHlg9iNo0LLK4czQqqO9A.png"><figcaption>This is where it all began..</figcaption></figure> <p>It may be divided into 2 parts. The first part, <strong><em>g </em></strong>takes an input (ahem dendrite ahem), performs an aggregation and based on the aggregated value the second part, <strong><em>f</em></strong> makes a decision.</p> <p>Lets suppose that I want to predict my own decision, whether to watch a random football game or not on TV. The inputs are all boolean i.e., {0,1} and my output variable is also boolean {0: Will watch it, 1: Won’t watch it}.</p> <ul> <li>So,<strong><em> x_1</em></strong> could be <em>isPremierLeagueOn </em>(I like Premier League more)</li> <li> <strong><em>x_2</em></strong> could be <em>isItAFriendlyGame</em> (I tend to care less about the friendlies)</li> <li> <strong><em>x_3 </em></strong>could be <em>isNotHome</em><strong> </strong>(Can’t watch it when I’m running errands. Can I?)</li> <li> <strong><em>x_4</em></strong> could be <em>isManUnitedPlaying</em><strong> </strong>(I am a big Man United fan. GGMU!) and so on.</li> </ul> <p>These inputs can either be <em>excitatory</em> or <em>inhibitory</em>. Inhibitory inputs are those that have maximum effect on the decision making irrespective of other inputs i.e., if <strong><em>x_3</em></strong> is 1 (not home) then my output will always be 0 i.e., the neuron will never fire, so <strong><em>x_3</em></strong> is an inhibitory input. Excitatory inputs are NOT the ones that will make the neuron fire on their own but they might fire it when combined together. Formally, this is what is going on:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/550/1*NLchBzohJvCCNMPPnF-V-A.png"></figure> <p>We can see that <strong><em>g</em>(x)</strong> is just doing a sum of the inputs — a simple aggregation. And <strong><em>theta</em></strong> here is called thresholding parameter. For example, if I always watch the game when the sum turns out to be 2 or more, the <strong><em>theta</em> </strong>is 2 here. This is called the Thresholding Logic.</p> <h3>Boolean Functions Using M-P Neuron</h3> <p>So far we have seen how the M-P neuron works. Now lets look at how this very neuron can be used to represent a few boolean functions. Mind you that our inputs are all boolean and the output is also boolean so essentially, the neuron is just trying to learn a boolean function. A lot of boolean decision problems can be cast into this, based on appropriate input variables— like whether to continue reading this post, whether to watch Friends after reading this post etc. can be represented by the M-P neuron.</p> <h4>M-P Neuron: A Concise Representation</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/393/1*UdKG02Hru_6VLv7ifs0K5g.png"></figure> <p>This representation just denotes that, for the boolean inputs <strong><em>x_1</em></strong>, <strong><em>x_2</em></strong> and <strong><em>x_3</em></strong> if the <strong><em>g</em>(x)</strong> i.e., <strong>sum</strong> <strong>≥</strong> <strong>theta</strong>, the neuron will fire otherwise, it won’t.</p> <h4><strong>AND Function</strong></h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/365/1*x8Hodz9QdjTitmL8vqbJRQ.png"></figure> <p>An AND function neuron would only fire when ALL the inputs are ON i.e., <strong><em>g</em>(x)</strong> ≥ 3 here.</p> <h4><strong>OR Function</strong></h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/387/1*G_ZNkjfUytkVEH8MncQIig.png"></figure> <p>I believe this is self explanatory as we know that an OR function neuron would fire if ANY of the inputs is ON i.e., <strong><em>g</em>(x)</strong> ≥ 1 here.</p> <h4><strong>A Function With An Inhibitory Input</strong></h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/396/1*0FisbY6LROsXyMXPlTaMgQ.png"></figure> <p>Now this might look like a tricky one but it’s really not. Here, we have an inhibitory input i.e., <strong><em>x_2</em></strong> so whenever <strong><em>x_2 </em></strong>is 1, the output will be<em> </em>0. Keeping that in mind, we know that <strong><em>x_1 AND !x_2</em></strong> would output 1 only when <strong><em>x_1</em></strong> is 1 and <strong><em>x_2</em></strong> is 0 so it is obvious that the threshold parameter should be 1.</p> <p>Lets verify that, the <strong><em>g</em>(x)</strong> i.e., <strong><em>x_1</em></strong> + <strong><em>x_2</em></strong> would be ≥ 1 in only 3 cases:</p> <p>Case 1: when <strong><em>x_1</em></strong> is 1 and <strong><em>x_2</em></strong> is 0<br>Case 2: when <strong><em>x_1</em></strong> is 1 and <strong><em>x_2</em></strong> is 1<br>Case 3: when <strong><em>x_1</em></strong> is 0 and <strong><em>x_2</em></strong> is 1</p> <p>But in both Case 2 and Case 3, we know that the output will be 0 because <strong><em>x_2 </em></strong>is 1 in both of them, thanks to the inhibition. And we also know that <strong><em>x_1 AND !x_2</em></strong> would output 1 for Case 1 (above) so our thresholding parameter holds good for the given function.</p> <h4><strong>NOR Function</strong></h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/390/1*9HGZhDjTStga79VvACBegw.png"></figure> <p>For a NOR neuron to fire, we want ALL the inputs to be 0 so the thresholding parameter should also be 0 and we take them all as inhibitory input.</p> <h4><strong>NOT Function</strong></h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/384/1*_KXA7o3LpPOIWiZL1vrCpg.png"></figure> <p>For a NOT neuron, 1 outputs 0 and 0 outputs 1. So we take the input as an inhibitory input and set the thresholding parameter to 0. It works!</p> <p>Can any boolean function be represented using the M-P neuron? Before you answer that, lets understand what M-P neuron is doing geometrically.</p> <h3>Geometric Interpretation Of M-P Neuron</h3> <p>This is the best part of the post according to me. Lets start with the OR function.</p> <h4>OR Function</h4> <p>We already discussed that the OR function’s thresholding parameter <strong><em>theta</em></strong><em> </em>is 1, for obvious reasons. The inputs are obviously boolean, so only 4 combinations are possible — (0,0), (0,1), (1,0) and (1,1). Now plotting them on a 2D graph and making use of the OR function’s aggregation equation <br>i.e., <strong><em>x_1 + x_2 </em>≥<em> </em>1 </strong>using which we can draw the decision boundary as shown in the graph below. Mind you again, this is not a real number graph.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*lupe8zIYqNb8t72XQIZDHw.png"></figure> <p>We just used the aggregation equation i.e., <strong><em>x_1 + x_2 =</em>1 </strong>to graphically show that all those inputs whose output when passed through the OR function M-P neuron lie ON or ABOVE that line and all the input points that lie BELOW that line are going to output 0.</p> <p>Voila!! The M-P neuron just learnt a linear decision boundary! The M-P neuron is splitting the input sets into two classes — positive and negative. Positive ones (which output 1) are those that lie ON or ABOVE the decision boundary and negative ones (which output 0) are those that lie BELOW the decision boundary.</p> <p>Lets convince ourselves that the M-P unit is doing the same for all the boolean functions by looking at more examples (if it is not already clear from the math).</p> <h4>AND Function</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ErKKEr1uMBht6474K7Vf4g.png"></figure> <p>In this case, the decision boundary equation is <strong><em>x_1 + x_2 =</em>2</strong>. Here, all the input points that lie ON or ABOVE, just (1,1), output 1 when passed through the AND function M-P neuron. It fits! The decision boundary works!</p> <h4><strong>Tautology</strong></h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*9P5f7cT0DEnqeVwuDfHwJA.png"><figcaption>Too easy, right?</figcaption></figure> <p>I think you get it by now but what if we have more than 2 inputs?</p> <h4>OR Function With 3 Inputs</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*2L-R-E1upw2JcYv-B2zU4A.png"></figure> <p>Lets just generalize this by looking at a 3 input OR function M-P unit. In this case, the possible inputs are 8 points — (0,0,0), (0,0,1), (0,1,0), (1,0,0), (1,0,1),… you got the point(s). We can map these on a 3D graph and this time we draw a decision boundary in 3 dimensions.</p> <blockquote>“Is it a bird? Is it a plane?”</blockquote> <p>Yes, it is a PLANE!</p> <p>The plane that satisfies the decision boundary equation <strong><em>x_1 + x_2 + x_3 = </em>1 </strong>is shown below:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/561/1*e-iWzKyf8wr99jXbl66nxA.png"></figure> <p>Take your time and convince yourself by looking at the above plot that all the points that lie ON or ABOVE that plane (positive half space) will result in output 1 when passed through the OR function M-P unit and all the points that lie BELOW that plane (negative half space) will result in output 0.</p> <p>Just by hand coding a thresholding parameter, M-P neuron is able to conveniently represent the boolean functions which are linearly separable.</p> <p><em>Linear separability (for boolean functions): There exists a line (plane) such that all inputs which produce a 1 lie on one side of the line (plane) and all inputs which produce a 0 lie on other side of the line (plane).</em></p> <h3>Limitations Of M-P Neuron</h3> <ul> <li>What about non-boolean (say, real) inputs?</li> <li>Do we always need to hand code the threshold?</li> <li>Are all inputs equal? What if we want to assign more importance to some inputs?</li> <li>What about functions which are not linearly separable? Say XOR function.</li> </ul> <p>I hope it is now clear why we are not using the M-P neuron today. Overcoming the limitations of the M-P neuron, Frank Rosenblatt, an American psychologist, proposed the classical perception model, the mighty <em>artificial neuron</em>, in 1958. It is more generalized computational model than the McCulloch-Pitts neuron where weights and thresholds can be learnt over time.</p> <p>More on <em>perceptron</em> and how it learns the weights and thresholds etc. in my future posts.</p> <h3>Conclusion</h3> <p>In this article, we briefly looked at biological neurons. We then established the concept of MuCulloch-Pitts neuron, the first ever mathematical model of a biological neuron. We represented a bunch of boolean functions using the M-P neuron. We also tried to get a geometric intuition of what is going on with the model, using 3D plots. In the end, we also established a motivation for a more generalized model, the one and only <em>artificial neuron/perceptron</em> model.</p> <p>Thank you for reading the article.<br>Live and let live!<br>A</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=5fdf65ac5dd1" width="1" height="1" alt="">&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/towards-data-science/mcculloch-pitts-model-5fdf65ac5dd1" rel="external nofollow noopener" target="_blank">McCulloch-Pitts Neuron — Mankind’s First Mathematical Model Of A Biological Neuron</a> was originally published in <a href="https://medium.com/towards-data-science" rel="external nofollow noopener" target="_blank">TDS Archive</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p> </body></html>