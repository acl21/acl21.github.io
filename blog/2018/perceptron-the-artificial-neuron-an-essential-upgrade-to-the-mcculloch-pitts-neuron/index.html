<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>The most fundamental unit of a deep neural network is called an <em>artificial neuron</em>, which takes an input, processes it, passes it through an activation function like the <a href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="external nofollow noopener" target="_blank">Sigmoid</a>, return the activated output. In this post, we are only going to talk about the <em>perceptron</em> model proposed before the ‘activation’ part came into the picture.</p> <p>Frank Rosenblatt, an American psychologist, proposed the <em>classical perceptron</em> model in 1958. Further refined and carefully analyzed by <a href="http://science.sciencemag.org/content/165/3895/780" rel="external nofollow noopener" target="_blank">Minsky and Papert</a> (1969) — their model is referred to as the <em>perceptron</em> model. This is a follow-up post to my previous post on <a href="https://towardsdatascience.com/mcculloch-pitts-model-5fdf65ac5dd1" rel="external nofollow noopener" target="_blank">McCulloch-Pitts neuron</a>, I suggest you at least quickly skim through it to better appreciate the Minsky-Papert contributions.</p> <p><em>Citation Note: The concept, the content, and the structure of this article were inspired by the awesome lectures and the material offered by Prof. </em><a href="https://www.cse.iitm.ac.in/~miteshk/" rel="external nofollow noopener" target="_blank"><strong><em>Mitesh M. Khapra</em></strong></a><em> on </em><a href="http://nptel.ac.in" rel="external nofollow noopener" target="_blank"><em>NPTEL</em></a><em>’s </em><a href="https://onlinecourses.nptel.ac.in/noc18_cs41/preview" rel="external nofollow noopener" target="_blank"><em>Deep Learning</em></a><em> course. Check it out!</em></p> <h3>Perceptron</h3> <figure><img alt="" src="https://cdn-images-1.medium.com/max/645/1*-JtN9TWuoZMz7z9QKbT85A.png"></figure> <p>The <em>perceptron </em>model, proposed by Minsky-Papert, is a more general computational model than McCulloch-Pitts neuron. It overcomes some of the limitations of the M-P neuron by introducing the concept of numerical weights (a measure of importance) for inputs, and a mechanism for learning those weights. Inputs are no longer limited to boolean values like in the case of an M-P neuron, it supports real inputs as well which makes it more useful and generalized.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/870/1*Fyapb-JRFJ-VtnLYLLXCwg.png"></figure> <p>Now, this is very similar to an M-P neuron but we take a weighted sum of the inputs and set the output as one only when the sum is more than an arbitrary threshold (<strong><em>theta</em></strong>). However, according to the convention, instead of hand coding the thresholding parameter <strong><em>thetha</em></strong>, we add it as one of the inputs, with the weight -<strong><em>theta</em></strong> like shown below, which makes it learn-able (more on this in my next post — <em>Perceptron Learning Algorithm</em>).</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/870/1*gKFs7YU44vJFiS2rF3-bpg.png"></figure> <p>Consider the task of predicting whether I would watch a random game of football on TV or not (the same example from my M-P neuron post) using the behavioral data available. And let's assume my decision is solely dependent on 3 binary inputs (binary for simplicity).</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/859/1*GJp3269Q0pY1Lg54DHbteQ.png"></figure> <p>Here, <strong><em>w_0</em></strong> is called the bias because it represents the prior (prejudice). A football freak may have a very low threshold and may watch any football game irrespective of the league, club or importance of the game [<strong><em>theta = 0</em></strong>]. On the other hand, a selective viewer like me may only watch a football game that is a premier league game, featuring Man United game and is not friendly [<strong><em>theta = 2</em></strong>]. The point is, the <strong>weights</strong> and the <strong>bias </strong>will depend on the data (my viewing history in this case).</p> <p>Based on the data, if needed the model may have to give a lot of importance (high weight) to the <em>isManUnitedPlaying </em>input and penalize the weights of other inputs.</p> <h3>Perceptron vs McCulloch-Pitts Neuron</h3> <p>What kind of functions can be implemented using a <em>perceptron</em>? How different is it from McCulloch-Pitts neurons?</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/759/1*SzzZhOB-xHyOchebT9Fv7w.png"></figure> <p>From the equations, it is clear that even a <em>perceptron </em>separates the input space into two halves, positive and negative. All the inputs that produce an output 1 lie on one side (positive half space) and all the inputs that produce an output 0 lie on the other side (negative half space).</p> <p>In other words, a single <em>perceptron</em> can only be used to implement <strong>linearly separable</strong> functions, just like the M-P neuron. Then what is the difference? Why do we claim that the <em>perceptron</em> is an updated version of an M-P neuron? Here, the weights, including the threshold can be <strong>learned</strong> and the inputs can be <strong>real</strong> values.</p> <h3>Boolean Functions Using Perceptron</h3> <h4>OR Function — Can Do!</h4> <p>Just revisiting the good old OR function the <em>perceptron</em> way.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/910/1*C5LeL8JDfoGbkUg0cu1M-w.png"><figcaption>Try solving the equations on your own.</figcaption></figure> <p>The above ‘possible solution’ was obtained by solving the linear system of equations on the left. It is clear that the solution separates the input space into two spaces, negative and positive half spaces. I encourage you to try it out for AND and other boolean function.</p> <p>Now if you actually try and solve the linear equations above, you will realize that there can be multiple solutions. But which solution is the best? To more formally define the ‘best’ solution, we need to understand errors and error surfaces, which we will do in my next post on <em>Perceptron Learning Algorithm.</em></p> <h4>XOR Function — Can’t Do!</h4> <p>Now let's look at a non-linear boolean function i.e., you cannot draw a line to separate positive inputs from the negative ones.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/924/1*E7YhKsJ2wb-VdeXpg89lvg.png"></figure> <p>Notice that the fourth equation contradicts the second and the third equation. Point is, there are no <em>perceptron </em>solutions for non-linearly separated data. So the key take away is that a <strong>single</strong> <em>perceptron</em> cannot learn to separate the data that are non-linear in nature.</p> <blockquote><strong>The XOR Affair</strong></blockquote> <blockquote>In the book published by <a href="http://science.sciencemag.org/content/165/3895/780" rel="external nofollow noopener" target="_blank">Minsky and Papert</a> in 1969, the authors implied that, since a single artificial neuron is incapable of implementing some functions such as the <a href="https://en.wikipedia.org/wiki/XOR" rel="external nofollow noopener" target="_blank">XOR</a> logical function, larger networks also have similar limitations, and therefore should be dropped. Later research on three-layered perceptrons showed how to implement such functions, therefore saving the technique from obliteration.</blockquote> <blockquote>— <em>Wikipedia</em> </blockquote> <h3>(Optional) Motivation For Sigmoid Neurons</h3> <p>As I mentioned earlier, the artificial neurons we use today are slightly different from the <em>perceptron</em> we looked at, the difference is the activation function. here. Some might say that the thresholding logic used by a <em>perceptron</em> is very harsh. For example, if you look at a problem of deciding if I will be watching a movie or not, based only on one real-valued input (<strong><em>x_1</em></strong> = <em>criticsRating</em>) and if the threshold we set is 0.5 (<strong><em>w_0</em></strong> = -0.5) and <strong><em>w_1</em></strong>= 1 then our setup would look like this:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/383/1*oPtJQSy2JkluyWgU6f55MQ.png"></figure> <p>What would be the decision for a movie with <em>criticsRating</em> = 0.51? <em>Yes!<br></em>What would be the decision for a movie with <em>criticsRating</em> = 0.49? <em>No!<br></em>Some might say that its harsh that we would watch a movie with a rating of 0.51 but not the one with a rating of 0.49 and this is where Sigmoid comes into the picture. Now convince yourself that this harsh thresholding is not attributed to just one specific problem we chose here, it could happen with any or every problem we deal with. It is a characteristic of the <em>perceptron</em> function itself which behaves like a step function.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/851/1*-qZhObF2Hp0Av68lDojaVA.png"></figure> <p>There will be this sudden change in the decision (from 0 to 1) when z value crosses the threshold (-<strong><em>w_0</em></strong>). For most real-world applications we would expect a smoother decision function which gradually changes from 0 to 1.</p> <p>Introducing sigmoid neurons where the output function is much smoother than the step function seems like a logical and obvious thing to do. Mind you that a sigmoid function is a mathematical function with a characteristic “S”-shaped curve, also called the <strong>sigmoid </strong>curve. There are many functions that can do the job for you, some are shown below:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/700/1*XPjkd29mA6P9lexeDI6g0Q.png"><figcaption>- <a href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="external nofollow noopener" target="_blank">Wikipedia</a></figcaption></figure> <p>One of the simplest one to work with is the logistic function.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/942/1*QLtPF0YkwYRLVoSeh2w4gw.png"><figcaption>Quick Question: What happens to <strong>y</strong> when <strong>z </strong>is infinite? Or when it is -infinite?</figcaption></figure> <p>We no longer see a sharp transition around the <strong><em>w_0</em></strong>. Also, the output is no longer binary but a real value between 0 and 1 which can be interpreted as a probability. So instead of yes/no decision, we get the probability of yes. The output here is <strong>smooth</strong>, <strong>continuous</strong> and <strong>differentiable </strong>and just how any learning algorithm likes it. To verify this yourself, please look through the <strong>backpropagation</strong> concept in Deep Learning.</p> <h3>Conclusion</h3> <p>In this post, we looked at a <em>perceptron</em>, the fundamental unit of deep neural networks. We also showed with examples how a <em>perceptron</em>, in contrast with the McCulloch-Pitts neuron, is more generalized and overcomes a few of the pertaining limitations at the time. We briefly established the motivation for Sigmoid neurons as well.</p> <p>In my next post, we will closely look at the famous <a href="https://towardsdatascience.com/d5db0deab975" rel="external nofollow noopener" target="_blank"><em>Perceptron Learning Algorithm</em></a> and try and get an intuition of why it works, without getting into any of the complex proofs, along with an implementation of the algorithm in Python from scratch.</p> <p>Thank you for reading the article.<br>Live and let live!<br>A</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*GF6ozztlIUgBB7wY8OtxLA.jpeg"><figcaption>Photo by <a href="https://unsplash.com/photos/BW0vK-FA3eg?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="external nofollow noopener" target="_blank">Clint Adair</a> on <a href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="external nofollow noopener" target="_blank">Unsplash</a></figcaption></figure> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=4d8c70d5cc8d" width="1" height="1" alt="">&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/data-science/perceptron-the-artificial-neuron-4d8c70d5cc8d" rel="external nofollow noopener" target="_blank">Perceptron: The Artificial Neuron (An Essential Upgrade To The McCulloch-Pitts Neuron)</a> was originally published in <a href="https://medium.com/data-science" rel="external nofollow noopener" target="_blank">TDS Archive</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p> </body></html>